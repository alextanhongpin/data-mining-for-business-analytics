{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hidden-necessity",
   "metadata": {},
   "source": [
    "# Classification Metrics\n",
    "\n",
    "\n",
    "- True Positive (TP) - a class is predicted true and is true in reality\n",
    "- True Negative (TN) - a class is predicted false and is false in reality\n",
    "- False Positive (FP) - a class is predicted true but is false in reality\n",
    "- False Negative (FN) - a class is predicted false but is true in reality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-senegal",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Accuracy is the number of correct predictions divided by the total number of predictions.\n",
    "\n",
    "$Accuracy = \\frac{TP + TN}{TP + FP + TN + FN}$\n",
    "\n",
    "- it ranges from 0 to 1, or 0 to 100%\n",
    "- not a reliable measure for imbalanced dataset, e.g. Given a dataset where 98% of samples of belongs to the class `healthy` and 2% to the class `sick`, the model can give 98% training accuracy by simply predicting every patient is healthy, even if they have serious illness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-arabic",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "Precision is the number of correct positive results divided by the total number of positive results.\n",
    "\n",
    "$Precision = \\frac{TP}{TP + FP}$\n",
    "\n",
    "\n",
    "- use precision when _false positive_ cannot be ignored.\n",
    "- e.g. a spam detector model. It is okay if the model sends a couple of spam letters to the inbox, but sending an important non-span email to the spam folder (false positive) is much worse\n",
    "- use precision for imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-leone",
   "metadata": {},
   "source": [
    "## Recall\n",
    "\n",
    "Recall is the number of correct positives divided by the number of all positives.\n",
    "\n",
    "$Recall = \\frac{TP}{TP + FN}$\n",
    "\n",
    "\n",
    "- use recall when the FN cannot be ignored, e.g. when you model is meant to predict sick people, but it predicts sick people as healthy (false negative) and left them untreated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-vietnam",
   "metadata": {},
   "source": [
    "## F1 Score\n",
    "\n",
    "F1 Score tries to find the balance between precision and recall by calculating their harmonic mean. F1 Score of 1 indicates perfect precision and recall.\n",
    "\n",
    "$F1 Score = \\text{Harmonic mean of Precision and Recall} = 2 x \\frac{precision x recall}{precision + recall} = \\frac{2TP}{2TP + FP + FN}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-bench",
   "metadata": {},
   "source": [
    "## Specificity\n",
    "\n",
    "Specificity is the number of correctly predicted negatives out of all the negatives.\n",
    "\n",
    "$Specificity = \\frac{TN}{TN + FP}$\n",
    "\n",
    "- the opposite of recall\n",
    "- when false positive cannot be tolerated, e.g. in a fraud detection model, those flagged as fraudulent (positive) could be put to jail. Since we cannot afford to put any innocent people to jail, false positive is unacceptable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
